{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "b2e0177e202a493f880b1620c15a2263": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_00f82e2803a3473bbad6af1b0d911c86",
              "IPY_MODEL_fb9c0ab5f4654e93bdf27b598df85a98",
              "IPY_MODEL_3724858f90da460eac48cfba139d3e75"
            ],
            "layout": "IPY_MODEL_15b55c177ed34d2f82624f46d218022c"
          }
        },
        "00f82e2803a3473bbad6af1b0d911c86": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3435dd8abeb84a2c8d0b29aacdf7fd17",
            "placeholder": "​",
            "style": "IPY_MODEL_ce1e4a584de842d2a0e9d228bd8c0701",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "fb9c0ab5f4654e93bdf27b598df85a98": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8b5a8cc103df461f85e12ac8b411177b",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_38002f1256104a3f9b0d4ac39d8eaf4a",
            "value": 3
          }
        },
        "3724858f90da460eac48cfba139d3e75": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e2cbfa916d554d4a9fddd55ef68239b3",
            "placeholder": "​",
            "style": "IPY_MODEL_55fc15439d124048b4df0fcc368a2f92",
            "value": " 3/3 [01:34&lt;00:00, 31.36s/it]"
          }
        },
        "15b55c177ed34d2f82624f46d218022c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3435dd8abeb84a2c8d0b29aacdf7fd17": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ce1e4a584de842d2a0e9d228bd8c0701": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8b5a8cc103df461f85e12ac8b411177b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "38002f1256104a3f9b0d4ac39d8eaf4a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e2cbfa916d554d4a9fddd55ef68239b3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "55fc15439d124048b4df0fcc368a2f92": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Install required libraries\n",
        "import bitsandbytes\n",
        "import pandas as pd\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pydantic import BaseModel\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "from scipy import stats\n",
        "from typing import Callable, Dict\n",
        "from datetime import datetime\n",
        "import gradio as gr\n",
        "import json\n",
        "import re\n"
      ],
      "metadata": {
        "id": "uZQ0Ys8_GnwS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hugging face login\n",
        "from huggingface_hub import login\n",
        "HF_TOKEN = \"Place your actual token\"\n",
        "login(HF_TOKEN)\n"
      ],
      "metadata": {
        "id": "xX6S5j72Vavi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Configure Mistral-7B-Instruct with 4-bit Quantization\n",
        "\n",
        "# Enable GPU if available\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# 4-bit quantization config\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16\n",
        ")\n",
        "\n",
        "# Load model and tokenizer\n",
        "model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    token=HF_TOKEN\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, token=HF_TOKEN)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "b2e0177e202a493f880b1620c15a2263",
            "00f82e2803a3473bbad6af1b0d911c86",
            "fb9c0ab5f4654e93bdf27b598df85a98",
            "3724858f90da460eac48cfba139d3e75",
            "15b55c177ed34d2f82624f46d218022c",
            "3435dd8abeb84a2c8d0b29aacdf7fd17",
            "ce1e4a584de842d2a0e9d228bd8c0701",
            "8b5a8cc103df461f85e12ac8b411177b",
            "38002f1256104a3f9b0d4ac39d8eaf4a",
            "e2cbfa916d554d4a9fddd55ef68239b3",
            "55fc15439d124048b4df0fcc368a2f92"
          ]
        },
        "id": "SUTk_giiBnSM",
        "outputId": "6beff265-a705-4aee-d60d-4c45dd579e13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b2e0177e202a493f880b1620c15a2263"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to call the LLM\n",
        "def call_llm(prompt: str, max_tokens: int = 500) -> str:\n",
        "    # Construct a prompt that strongly emphasizes JSON response format\n",
        "    full_prompt = (\n",
        "        f\"[INST] {prompt}\\n\\n\"\n",
        "        f\"IMPORTANT: Your response must be a valid JSON object only. Format your response as proper JSON.\\n\"\n",
        "        f\"DO NOT include any text outside the JSON object.\\n\"\n",
        "        f\"Example of correct format: {{\\\"key\\\": \\\"value\\\", \\\"otherKey\\\": 123}}\\n\"\n",
        "        f\"Do not use markdown formatting for JSON. [/INST]\"\n",
        "    )\n",
        "\n",
        "    # Tokenize and generate response\n",
        "    inputs = tokenizer(full_prompt, return_tensors=\"pt\").to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_tokens,\n",
        "            temperature=0.1,  # Lower temperature for more predictable outputs\n",
        "            top_p=0.95,\n",
        "            do_sample=True,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "    # Decode the response\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    # Extract the part after [/INST]\n",
        "    if \"[/INST]\" in response:\n",
        "        response = response.split(\"[/INST]\", 1)[1].strip()\n",
        "\n",
        "    return response\n"
      ],
      "metadata": {
        "id": "ZAn-OPaHgbOt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Defining Tools\n",
        "\n",
        "# Tool 1: Load and Clean Dataset\n",
        "def load_and_clean_data(file_path: str) -> dict:\n",
        "    try:\n",
        "        print(f\"Attempting to load data from {file_path}\")\n",
        "        df = pd.read_csv(file_path)\n",
        "        initial_rows = df.shape[0]\n",
        "\n",
        "        # Cleaning\n",
        "        df = df.drop_duplicates()  # Remove duplicates\n",
        "        df = df.dropna(how='all')  # Drop rows with all NaN\n",
        "        # Impute missing values for numeric columns with median\n",
        "        numeric_cols = df.select_dtypes(include=['float64', 'int64']).columns\n",
        "        for col in numeric_cols:\n",
        "            df[col] = df[col].fillna(df[col].median())\n",
        "        # Convert object columns to categorical if unique values < 50%\n",
        "        for col in df.select_dtypes(include=['object']).columns:\n",
        "            if df[col].nunique() / len(df) < 0.5:\n",
        "                df[col] = df[col].astype('category')\n",
        "        # Remove outliers using IQR method\n",
        "        for col in numeric_cols:\n",
        "            Q1 = df[col].quantile(0.25)\n",
        "            Q3 = df[col].quantile(0.75)\n",
        "            IQR = Q3 - Q1\n",
        "            df = df[~((df[col] < (Q1 - 1.5 * IQR)) | (df[col] > (Q3 + 1.5 * IQR)))]\n",
        "\n",
        "        cleaned_rows = df.shape[0]\n",
        "        return {\n",
        "            \"data\": df,\n",
        "            \"status\": \"success\",\n",
        "            \"message\": f\"Loaded and cleaned dataset from {file_path}. Rows: {initial_rows} -> {cleaned_rows}. Removed duplicates, imputed missing values, and handled outliers.\"\n",
        "        }\n",
        "    except Exception as e:\n",
        "        return {\"data\": None, \"status\": \"error\", \"message\": f\"Failed to load/clean data: {str(e)}\"}\n",
        "\n",
        "# Tool 2: Perform EDA\n",
        "def perform_eda(df: pd.DataFrame) -> dict:\n",
        "    try:\n",
        "        if df is None or df.empty:\n",
        "            return {\"status\": \"error\", \"message\": \"No data available for EDA.\", \"summary_stats\": {}, \"skewness\": {}, \"missing_values\": {}, \"outliers\": {}}\n",
        "\n",
        "        # Summary statistics\n",
        "        summary_stats = df.describe(include='all').to_dict()\n",
        "\n",
        "        # Missing values\n",
        "        missing_values = df.isnull().sum().to_dict()\n",
        "\n",
        "        # Skewness for numeric columns\n",
        "        numeric_cols = df.select_dtypes(include=['float64', 'int64']).columns\n",
        "        skewness = {col: float(stats.skew(df[col].dropna())) for col in numeric_cols}\n",
        "\n",
        "        # Outlier detection using IQR\n",
        "        outliers = {}\n",
        "        for col in numeric_cols:\n",
        "            Q1 = df[col].quantile(0.25)\n",
        "            Q3 = df[col].quantile(0.75)\n",
        "            IQR = Q3 - Q1\n",
        "            outliers[col] = len(df[(df[col] < (Q1 - 1.5 * IQR)) | (df[col] > (Q3 + 1.5 * IQR))])\n",
        "\n",
        "        # Correlation matrix for numeric columns\n",
        "        corr_matrix = df[numeric_cols].corr().to_dict()\n",
        "\n",
        "        return {\n",
        "            \"status\": \"success\",\n",
        "            \"summary_stats\": summary_stats,\n",
        "            \"skewness\": skewness,\n",
        "            \"missing_values\": missing_values,\n",
        "            \"outliers\": outliers,\n",
        "            \"corr_matrix\": corr_matrix,\n",
        "            \"message\": \"Comprehensive EDA completed.\"\n",
        "        }\n",
        "    except Exception as e:\n",
        "        return {\"status\": \"error\", \"message\": f\"EDA failed: {str(e)}\", \"summary_stats\": {}, \"skewness\": {}, \"missing_values\": {}, \"outliers\": {}, \"corr_matrix\": {}}\n",
        "\n",
        "# Tool 3: Generate Visualizations\n",
        "def generate_plot(df: pd.DataFrame, plot_type: str = \"auto\", x_col: str = None, y_col: str = None, annot: bool = False, **kwargs) -> dict:\n",
        "    numeric_cols = df.select_dtypes(include=['float64', 'int64']).columns\n",
        "    categorical_cols = df.select_dtypes(include=['category', 'object']).columns\n",
        "\n",
        "    # Auto-select plot type if not specified\n",
        "    if plot_type == \"auto\":\n",
        "        if x_col and y_col:\n",
        "            plot_type = \"scatter\" if y_col in numeric_cols else \"bar\"\n",
        "        elif x_col:\n",
        "            plot_type = \"histogram\" if x_col in numeric_cols else \"count\"\n",
        "        else:\n",
        "            plot_type = \"heatmap\"  # Default to correlation heatmap\n",
        "\n",
        "    # Validate columns\n",
        "    if x_col and x_col not in df.columns:\n",
        "        return {\"status\": \"error\", \"message\": f\"Column '{x_col}' not found in dataset.\"}\n",
        "    if y_col and y_col not in df.columns:\n",
        "        return {\"status\": \"error\", \"message\": f\"Column '{y_col}' not found in dataset.\"}\n",
        "\n",
        "    plt.figure(figsize=(16, 10))  # Increased size for better visibility\n",
        "    try:\n",
        "        if plot_type.lower() == \"histogram\" and x_col:\n",
        "            sns.histplot(data=df, x=x_col, kde=True)\n",
        "        elif plot_type.lower() == \"scatter\" and x_col and y_col:\n",
        "            sns.scatterplot(data=df, x=x_col, y=y_col, hue=categorical_cols[0] if len(categorical_cols) > 0 else None)\n",
        "        elif plot_type.lower() == \"line\" and x_col and y_col:\n",
        "            sns.lineplot(data=df, x=x_col, y=y_col)\n",
        "        elif plot_type.lower() == \"box\" and x_col:\n",
        "            # Use y=x_col for vertical box plot to show distribution\n",
        "            sns.boxplot(data=df, y=x_col, color='skyblue')\n",
        "            # Add grid for better readability\n",
        "            plt.grid(True, axis='y', linestyle='--', alpha=0.7)\n",
        "        elif plot_type.lower() == \"count\" and x_col:\n",
        "            sns.countplot(data=df, x=x_col)\n",
        "        elif plot_type.lower() == \"heatmap\":\n",
        "            if x_col or y_col:\n",
        "                return {\"status\": \"error\", \"message\": \"Heatmap does not use specific x_col or y_col; it shows correlation matrix.\"}\n",
        "\n",
        "            # Compute correlation\n",
        "            corr_matrix = df[numeric_cols].corr()\n",
        "\n",
        "            # Generate heatmap\n",
        "            sns.heatmap(corr_matrix, annot=annot, cmap=\"coolwarm\", linewidths=0.5, vmin=-1, vmax=1,\n",
        "                        square=True, cbar_kws={\"shrink\": 0.75})\n",
        "\n",
        "            # Fix x and y labels for better readability\n",
        "            plt.xticks(rotation=45, ha=\"right\", fontsize=10)\n",
        "            plt.yticks(fontsize=10)\n",
        "\n",
        "        else:\n",
        "            return {\"status\": \"error\", \"message\": f\"Invalid plot type '{plot_type}' or missing required columns.\"}\n",
        "\n",
        "        plt.title(f\"{plot_type.capitalize()} Plot: {x_col if x_col else 'Heatmap'} {f'vs {y_col}' if y_col else ''}\", fontsize=14, pad=20)\n",
        "        plt.tight_layout()\n",
        "\n",
        "        # Save plot\n",
        "        file_name = f\"plot_{plot_type}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.png\"\n",
        "        plt.savefig(file_name)\n",
        "        plt.close()\n",
        "\n",
        "        return {\"status\": \"success\", \"message\": f\"Generated {plot_type} plot.\", \"file\": file_name}\n",
        "\n",
        "    except Exception as e:\n",
        "        return {\"status\": \"error\", \"message\": f\"Plot generation failed: {str(e)}\"}\n",
        "\n",
        "# Tool 4: Correlation Analysis\n",
        "def calculate_correlation(df: pd.DataFrame, col1: str = None, col2: str = None, **kwargs) -> dict:\n",
        "    try:\n",
        "        numeric_cols = df.select_dtypes(include=['float64', 'int64']).columns\n",
        "        if not col1 or not col2:\n",
        "            col1, col2 = numeric_cols[:2] if len(numeric_cols) >= 2 else (None, None)\n",
        "        if col1 not in numeric_cols or col2 not in numeric_cols:\n",
        "            return {\"status\": \"error\", \"message\": \"Columns must be numeric for correlation.\"}\n",
        "\n",
        "        corr = df[col1].corr(df[col2])\n",
        "        corr_matrix = df[numeric_cols].corr().to_dict()\n",
        "        return {\n",
        "            \"correlation\": float(corr),\n",
        "            \"col1\": col1,\n",
        "            \"col2\": col2,\n",
        "            \"corr_matrix\": corr_matrix,\n",
        "            \"status\": \"success\",\n",
        "            \"message\": f\"Correlation between {col1} and {col2} calculated with full matrix.\"\n",
        "        }\n",
        "    except Exception as e:\n",
        "        return {\"status\": \"error\", \"message\": f\"Correlation failed: {str(e)}\", \"correlation\": None, \"corr_matrix\": {}}\n",
        "\n",
        "# Tool 5: Comprehensive Report Generation\n",
        "def generate_report(df: pd.DataFrame, eda_results: dict, analysis_results: dict, plot_results: list) -> dict:\n",
        "    report_lines = [\n",
        "        \"=== Data Science Report ===\",\n",
        "        f\"Generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\",\n",
        "        \"\\n1. Dataset Overview:\",\n",
        "        f\"  - Rows: {df.shape[0]}, Columns: {df.shape[1]}\",\n",
        "        f\"  - Columns: {', '.join(df.columns)}\",\n",
        "        \"\\n2. Exploratory Data Analysis:\",\n",
        "        f\"  - Summary Stats: {json.dumps(eda_results.get('summary_stats', {}), indent=2)}\",\n",
        "        f\"  - Missing Values: {json.dumps(eda_results.get('missing_values', {}), indent=2)}\",\n",
        "        f\"  - Skewness: {json.dumps(eda_results.get('skewness', {}), indent=2)}\",\n",
        "        f\"  - Outliers: {json.dumps(eda_results.get('outliers', {}), indent=2)}\",\n",
        "        \"\\n3. Correlation Analysis:\",\n",
        "        f\"  - Specific Correlation: {analysis_results.get('correlation', 'N/A')} between {analysis_results.get('col1', 'N/A')} and {analysis_results.get('col2', 'N/A')}\",\n",
        "        f\"  - Full Correlation Matrix: {json.dumps(analysis_results.get('corr_matrix', {}), indent=2)}\",\n",
        "        \"\\n4. Visualizations:\",\n",
        "        \"\\n  - \" + \"\\n  - \".join(plot_results or [\"No plots generated.\"]),\n",
        "        \"\\n=== End of Report ===\"\n",
        "    ]\n",
        "    return {\"report\": \"\\n\".join(report_lines), \"status\": \"success\", \"message\": \"Comprehensive report generated.\"}\n"
      ],
      "metadata": {
        "id": "Jeo7sqon6lvT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Defining Pydantic Model\n",
        "\n",
        "# Orchestrator Agent Output\n",
        "class OrchestratorOutput(BaseModel):\n",
        "    status: str\n",
        "    message: str\n",
        "    target_agent: str\n",
        "    parameters: dict\n"
      ],
      "metadata": {
        "id": "RkqQHazc7QXp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract json function\n",
        "def extract_json(response: str) -> dict:\n",
        "    try:\n",
        "        return json.loads(response)\n",
        "    except:\n",
        "        json_pattern = r'(\\{.*\\})'\n",
        "        match = re.search(json_pattern, response, re.DOTALL)\n",
        "        if match:\n",
        "            json_str = match.group(1)\n",
        "            json_str = re.sub(r\"'([^']*)':\", r'\"\\1\":', json_str)\n",
        "            json_str = re.sub(r'([{,])\\s*([a-zA-Z0-9_]+):', r'\\1\"\\2\":', json_str)\n",
        "            return json.loads(json_str)\n",
        "        return {}\n"
      ],
      "metadata": {
        "id": "qyRO7cZPofjX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Implement the Orchestrator Agent\n",
        "def orchestrator_agent(query: str, context: dict) -> OrchestratorOutput:\n",
        "    has_data = context.get(\"df\") is not None\n",
        "\n",
        "    prompt = (\n",
        "        f\"You are a data science orchestration system. Based on the user query, determine which \"\n",
        "        f\"specialized agent should handle the request.\\n\\n\"\n",
        "        f\"User query: \\\"{query}\\\"\\n\"\n",
        "        f\"Data loaded: {'Yes' if has_data else 'No'}\\n\\n\"\n",
        "        f\"Available agents:\\n\"\n",
        "        f\"- 'data': For loading datasets\\n\"\n",
        "        f\"- 'eda': For exploratory data analysis\\n\"\n",
        "        f\"- 'visualization': For creating plots\\n\"\n",
        "        f\"- 'analysis': For statistical analysis\\n\"\n",
        "        f\"- 'report': For generating reports (automatically perform prior steps if needed)\\n\\n\"\n",
        "        f\"Respond with a JSON object: {{'target_agent': 'name', 'message': 'explanation'}}\"\n",
        "    )\n",
        "\n",
        "    llm_response = call_llm(prompt)\n",
        "    json_response = extract_json(llm_response)\n",
        "\n",
        "    if not json_response or \"target_agent\" not in json_response:\n",
        "        return OrchestratorOutput(status=\"error\", message=\"Invalid request.\", target_agent=\"none\", parameters={})\n",
        "\n",
        "    target_agent = json_response.get(\"target_agent\", \"none\").lower()\n",
        "    message = json_response.get(\"message\", \"Processing...\")\n",
        "    parameters = {}\n",
        "\n",
        "    if target_agent == \"data\":\n",
        "        file_path_match = re.search(r'(/\\S+\\.\\w+)', query)\n",
        "        if file_path_match:\n",
        "            parameters[\"file_path\"] = file_path_match.group(1)\n",
        "    elif target_agent == \"visualization\":\n",
        "        # Extract plot type from query\n",
        "        plot_type = \"auto\"\n",
        "        if \"scatter\" in query.lower():\n",
        "            plot_type = \"scatter\"\n",
        "        elif \"line\" in query.lower():\n",
        "            plot_type = \"line\"\n",
        "        elif \"histogram\" in query.lower():\n",
        "            plot_type = \"histogram\"\n",
        "        elif \"box\" in query.lower():\n",
        "            plot_type = \"box\"\n",
        "        elif \"count\" in query.lower():\n",
        "            plot_type = \"count\"\n",
        "        elif \"heatmap\" in query.lower():\n",
        "            plot_type = \"heatmap\"\n",
        "\n",
        "        parameters[\"plot_type\"] = plot_type\n",
        "\n",
        "        # Extract column names from query (case-insensitive)\n",
        "        df_cols = context.get(\"df\", pd.DataFrame()).columns\n",
        "        query_lower = query.lower()\n",
        "        columns = []\n",
        "        for col in df_cols:\n",
        "            if col.lower() in query_lower:\n",
        "                columns.append(col)\n",
        "\n",
        "        if len(columns) >= 1:\n",
        "            parameters[\"x_col\"] = columns[0]\n",
        "        if len(columns) >= 2:\n",
        "            parameters[\"y_col\"] = columns[1]\n",
        "\n",
        "        # If no columns specified but plot type requires them, use defaults\n",
        "        if not parameters.get(\"x_col\") and plot_type in [\"scatter\", \"line\", \"histogram\", \"box\", \"count\"]:\n",
        "            numeric_cols = context[\"df\"].select_dtypes(include=['float64', 'int64']).columns\n",
        "            parameters[\"x_col\"] = numeric_cols[0] if len(numeric_cols) > 0 else None\n",
        "            if plot_type in [\"scatter\", \"line\"] and len(numeric_cols) > 1:\n",
        "                parameters[\"y_col\"] = numeric_cols[1]\n",
        "\n",
        "    elif target_agent == \"analysis\":\n",
        "        # Extract column names from query for correlation\n",
        "        df_cols = context.get(\"df\", pd.DataFrame()).columns\n",
        "        query_lower = query.lower()\n",
        "        columns = [col for col in df_cols if col.lower() in query_lower]\n",
        "        if len(columns) >= 2:\n",
        "            parameters[\"col1\"] = columns[0]\n",
        "            parameters[\"col2\"] = columns[1]\n",
        "\n",
        "    return OrchestratorOutput(status=\"success\", message=message, target_agent=target_agent, parameters=parameters)\n"
      ],
      "metadata": {
        "id": "zq-ZPeBOd5C-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main Inference with Gradio\n",
        "def run_data_science_assistant_gradio(query, file=None):\n",
        "    context = {\"df\": None, \"eda_results\": {}, \"analysis_results\": {}, \"plot_results\": []}\n",
        "    output_text = []\n",
        "    output_images = []\n",
        "\n",
        "    # Handle file upload if provided\n",
        "    if file is not None:\n",
        "        file_path = file.name  # Gradio uploads files to a temp location\n",
        "        result = load_and_clean_data(file_path)\n",
        "        if result[\"status\"] == \"success\":\n",
        "            context[\"df\"] = result[\"data\"]\n",
        "            output_text.append(f\"Data Agent: {result['message']}\")\n",
        "        else:\n",
        "            output_text.append(f\"Data Agent: {result['message']}\")\n",
        "            return \"\\n\".join(output_text), []\n",
        "\n",
        "    # Process the query if no file or after file is loaded\n",
        "    if query:\n",
        "        orch_output = orchestrator_agent(query, context)\n",
        "        output_text.append(f\"Orchestrator: {orch_output.message}\")\n",
        "        target_agent = orch_output.target_agent\n",
        "        parameters = orch_output.parameters\n",
        "\n",
        "        if target_agent == \"data\":\n",
        "            if file is None:\n",
        "                output_text.append(\"Assistant: Please upload a dataset file first.\")\n",
        "\n",
        "        elif target_agent == \"eda\":\n",
        "            if context[\"df\"] is None or context[\"df\"].empty:\n",
        "                output_text.append(\"Assistant: Please upload a dataset first.\")\n",
        "            else:\n",
        "                result = perform_eda(context[\"df\"])\n",
        "                context[\"eda_results\"] = result\n",
        "                output_text.append(f\"Assistant: {result['message']}\")\n",
        "                if result[\"status\"] == \"success\":\n",
        "                    output_text.append(\"Key Insights:\")\n",
        "                    for col, stats in result[\"summary_stats\"].items():\n",
        "                        if isinstance(stats, dict):\n",
        "                            output_text.append(f\"  {col}: mean={stats.get('mean', 'N/A'):.2f}, missing={result['missing_values'].get(col, 0)}\")\n",
        "\n",
        "        elif target_agent == \"visualization\":\n",
        "            if context[\"df\"] is None or context[\"df\"].empty:\n",
        "                output_text.append(\"Assistant: Please upload a dataset first.\")\n",
        "            else:\n",
        "                result = generate_plot(context[\"df\"], **parameters)\n",
        "                context[\"plot_results\"].append(result[\"message\"])\n",
        "                output_text.append(f\"Assistant: {result['message']}\")\n",
        "                if result[\"status\"] == \"success\":\n",
        "                    # Find the generated plot file\n",
        "                    import glob\n",
        "                    plot_files = glob.glob(f\"plot_{parameters.get('plot_type', 'auto')}*.png\")\n",
        "                    if plot_files:\n",
        "                        output_images.append(plot_files[-1])  # Add the latest plot\n",
        "\n",
        "        elif target_agent == \"analysis\":\n",
        "            if context[\"df\"] is None or context[\"df\"].empty:\n",
        "                output_text.append(\"Assistant: Please upload a dataset first.\")\n",
        "            else:\n",
        "                result = calculate_correlation(context[\"df\"], **parameters)\n",
        "                context[\"analysis_results\"] = result\n",
        "                if result[\"status\"] == \"success\":\n",
        "                    output_text.append(f\"Assistant: Correlation between {result['col1']} and {result['col2']} is {result['correlation']:.4f}\")\n",
        "                    output_text.append(\"Full Correlation Matrix:\")\n",
        "                    for col1, corrs in result[\"corr_matrix\"].items():\n",
        "                        for col2, corr in corrs.items():\n",
        "                            output_text.append(f\"  {col1} vs {col2}: {corr:.4f}\")\n",
        "                else:\n",
        "                    output_text.append(f\"Assistant: {result['message']}\")\n",
        "\n",
        "        elif target_agent == \"report\":\n",
        "            if context[\"df\"] is None or context[\"df\"].empty:\n",
        "                output_text.append(\"Assistant: No data loaded. Please upload a dataset first.\")\n",
        "            else:\n",
        "                if not context[\"eda_results\"]:\n",
        "                    context[\"eda_results\"] = perform_eda(context[\"df\"])\n",
        "                    output_text.append(\"Assistant: Performed EDA for report.\")\n",
        "                if not context[\"analysis_results\"]:\n",
        "                    context[\"analysis_results\"] = calculate_correlation(context[\"df\"])\n",
        "                    output_text.append(\"Assistant: Performed correlation analysis for report.\")\n",
        "                if not context[\"plot_results\"]:\n",
        "                    plot_result = generate_plot(context[\"df\"])\n",
        "                    context[\"plot_results\"].append(plot_result[\"message\"])\n",
        "                    output_text.append(\"Assistant: Generated a default plot for report.\")\n",
        "                    import glob\n",
        "                    plot_files = glob.glob(\"plot_auto*.png\")\n",
        "                    if plot_files:\n",
        "                        output_images.append(plot_files[-1])\n",
        "                result = generate_report(context[\"df\"], context[\"eda_results\"], context[\"analysis_results\"], context[\"plot_results\"])\n",
        "                output_text.append(f\"Assistant: {result['message']}\")\n",
        "                output_text.append(result[\"report\"])\n",
        "\n",
        "    return \"\\n\".join(output_text), output_images\n",
        "\n",
        "# Gradio Interface\n",
        "with gr.Blocks(title=\"Multi-Agent System for Data Analysis\") as demo:\n",
        "    gr.Markdown(\"# Multi-Agent System for Data Analysis\")\n",
        "    gr.Markdown(\"Upload a dataset and enter a query to perform data science tasks. Examples: 'perform EDA', 'scatter plot', 'correlation analysis', 'generate report'.\")\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column():\n",
        "            file_input = gr.File(label=\"Upload Dataset (CSV)\")\n",
        "            query_input = gr.Textbox(label=\"Enter your query\", placeholder=\"e.g., 'perform EDA' or 'scatter plot'\")\n",
        "            submit_btn = gr.Button(\"Submit\")\n",
        "        with gr.Column():\n",
        "            output_text = gr.Textbox(label=\"Output\", lines=20)\n",
        "            output_gallery = gr.Gallery(label=\"Visualizations\")\n",
        "\n",
        "    submit_btn.click(\n",
        "        fn=run_data_science_assistant_gradio,\n",
        "        inputs=[query_input, file_input],\n",
        "        outputs=[output_text, output_gallery]\n",
        "    )\n",
        "\n",
        "# Launch the Gradio app\n",
        "demo.launch(share=True)\n"
      ],
      "metadata": {
        "id": "W50n9mcpBaIE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 611
        },
        "outputId": "efbccbdb-156c-488e-9cc0-e75eb7fe0773"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://f11e5defca0b8ca5da.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://f11e5defca0b8ca5da.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7_hn3rSZJDWO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}